{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary modules from llama_index.core for indexing and storage management\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,        # Indexing class for storing vectors\n",
    "    SimpleDirectoryReader,   # Class for reading documents from a directory\n",
    "    StorageContext,          # Context manager for handling storage operations\n",
    "    ServiceContext,          # Context manager for handling service-related operations\n",
    "    load_index_from_storage  # Function to load an index from storage\n",
    ")\n",
    "\n",
    "# Importing specific classes for embeddings, node parsing, and LLM (Large Language Model) interaction\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding  # HuggingFace embedding model integration\n",
    "from llama_index.core.node_parser import SentenceSplitter            # Utility for splitting text into sentences\n",
    "from llama_index.llms.groq import Groq                               # Groq LLM interface for processing text\n",
    "\n",
    "import os  # Importing the OS module for operating system dependent functionality\n",
    "\n",
    "import warnings  # Importing warnings module to manage warning messages\n",
    "warnings.filterwarnings('ignore')  # Suppress warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a SimpleDirectoryReader to read our docker book\n",
    "reader = SimpleDirectoryReader(input_files=[\"docker for data science.pdf\"])\n",
    "\n",
    "# Load the data from the specified PDF file into the documents variable\n",
    "documents = reader.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of the pages in the book\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_label': 'v',\n",
       " 'file_name': 'docker for data science.pdf',\n",
       " 'file_path': 'docker for data science.pdf',\n",
       " 'file_type': 'application/pdf',\n",
       " 'file_size': 7272405,\n",
       " 'creation_date': '2024-08-10',\n",
       " 'last_modified_date': '2024-08-09'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the metadata of the 5th document in the loaded documents list\n",
    "documents[4].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b376f19de4a14df9bf6161abab031d2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/266 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize a SentenceSplitter with a chunk size of 1024 and a chunk overlap of 200\n",
    "text_splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=200)\n",
    "\n",
    "# Split the loaded documents into smaller chunks (nodes) based on sentences\n",
    "# The show_progress=True parameter enables progress display during the process\n",
    "nodes = text_splitter.get_nodes_from_documents(documents, show_progress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#length of nodes\n",
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_label': 'C1',\n",
       " 'file_name': 'docker for data science.pdf',\n",
       " 'file_path': 'docker for data science.pdf',\n",
       " 'file_type': 'application/pdf',\n",
       " 'file_size': 7272405,\n",
       " 'creation_date': '2024-08-10',\n",
       " 'last_modified_date': '2024-08-09'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the metadata of the 1st document in the loaded documents list\n",
    "nodes[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a HuggingFaceEmbedding model using the \"all-MiniLM-L6-v2\" model from the sentence-transformers library\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the GROQ API key from the environment variables\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Initialize a Groq model instance using the \"llama3-70b-8192\" model and the retrieved API key\n",
    "llm = Groq(model=\"llama3-70b-8192\", api_key=GROQ_API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ServiceContext using default settings and the specified embedding model and LLM\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a VectorStoreIndex from the list of documents\n",
    "# Parameters:\n",
    "# - documents: The book to be indexed\n",
    "# - show_progress=True: Display progress during the indexing process\n",
    "# - service_context: Provides the embedding model and LLM configuration for the index\n",
    "# - node_parser: The SentenceSplitter used to split the documents into chunks (nodes)\n",
    "vector_index = VectorStoreIndex.from_documents(\n",
    "    documents, \n",
    "    show_progress=True, \n",
    "    service_context=service_context, \n",
    "    node_parser=nodes\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Persist the current state of the VectorStoreIndex to a specified directory\n",
    "# This saves the index data to the \"./storage_mini\" directory for later retrieval\n",
    "vector_index.storage_context.persist(persist_dir=\"./storage_mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StorageContext from defaults, loading the index data from the specified directory\n",
    "# This context will be used to manage the storage operations, loading data from \"./storage_mini\"\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"./storage_mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the VectorStoreIndex from the specified storage context\n",
    "# Parameters:\n",
    "# - storage_context: Provides access to the storage directory (\"./storage_mini\") where the index data is stored\n",
    "# - service_context: Provides the embedding model and LLM configuration necessary for loading the index\n",
    "index = load_index_from_storage(storage_context, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a query engine from the loaded index using the specified service context\n",
    "# This engine will be used to perform queries on the index\n",
    "# Parameters:\n",
    "# - service_context: Provides the embedding model and LLM configuration needed for querying the index\n",
    "query_engine = index.as_query_engine(service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docker handles networking and isolation between containers using Linux network namespaces, which provide each container with its own isolated network stack, including IP addresses and routing tables. Containers can communicate through different network modes like Bridge (default), where they share a virtual network; Host, which removes network isolation; None, which isolates the container completely; and Overlay, which allows cross-host communication in a cluster. Dockerâ€™s built-in DNS facilitates service discovery, and firewall rules manage traffic, ensuring secure and controlled communication between containers and the outside world.\n"
     ]
    }
   ],
   "source": [
    "# Define a query to ask about Docker\n",
    "query = \"Explain how Docker handles networking and isolation between containers\"\n",
    "\n",
    "# Use the query engine to process the query and get a response\n",
    "# The query_engine utilizes the index and service context to generate an answer\n",
    "resp = query_engine.query(query)\n",
    "\n",
    "# Print the response from the query engine\n",
    "print(resp.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
